{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will be used to contain data processing components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T04:31:46.404814Z",
     "start_time": "2024-03-27T04:31:46.185715Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T04:31:46.419860Z",
     "start_time": "2024-03-27T04:31:46.406319Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can use the following to set the environment variables in the notebook if you don't set manually access key, secret key and endpoint in minio\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = os.getenv('MINIO_ACCESS_KEY')\n",
    "os.environ['AWS_SECRET_KEY'] = os.getenv('MINIO_SECRET_KEY')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = os.getenv('MINIO_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T04:31:46.434894Z",
     "start_time": "2024-03-27T04:31:46.422379Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define S3 storage\n",
    "obj_storage_access_key = os.getenv('MINIO_ACCESS_KEY')\n",
    "obj_storage_secret_key = os.getenv('MINIO_SECRET_KEY')\n",
    "obj_storage_endpoint = os.getenv('MINIO_STORAGE_ENDPOINT', 'http://minio:9000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T04:31:46.449993Z",
     "start_time": "2024-03-27T04:31:46.436895Z"
    }
   },
   "outputs": [],
   "source": [
    "path_1 = \"s3a://data/data-raw/data.json\"\n",
    "path_2 = \"s3a://data/data-raw/data2.json\"\n",
    "path_3 = \"s3a://data/data-raw/data3.json\"\n",
    "result_path = \"s3a://data/data-result/result.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# You need to more configuration if you want to use minio as object storage \n",
    "# (hint: maybe you can using .config() method to set the configuration if you want using spark to read/write data from/to minio)\n",
    "# Make sure Java install and JAVA_HOME as well as PATH is updated\n",
    "spark = (SparkSession.builder.appName(\"Python Spark SQL basic example\")\n",
    "         .getOrCreate())\n",
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"MINIO_ACCESS_KEY\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\n",
    "                                                 os.getenv(\"MINIO_SECRET_KEY\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", obj_storage_endpoint)\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "load_config(spark.sparkContext)\n",
    "\n",
    "\n",
    "# Read JSON file from MinIO\n",
    "data1 = spark.read.option(\"multiline\",\"true\").json(path_1)\n",
    "data2 = spark.read.option(\"multiline\",\"true\").json(path_2)\n",
    "data3 = spark.read.option(\"multiline\",\"true\").json(path_3)\n",
    "# Merge data from multiple dataframe\n",
    "data = data1.union(data2).union(data3)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dropDuplicates to remove duplicated rows and show\n",
    "data = data.dropDuplicates()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Update load result to MinIO with json format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Based on the fact that Spark save data in multiple parts to allow multiple workers can access and write in parallel\n",
    "# To save result in single file, I chose save dataframe to json file then upload to MinIO\n",
    "import json, shutil, os\n",
    "from ultils import upload_file_to_minio\n",
    "json_data = data.toJSON().collect()\n",
    "json_data = [json.loads(x) for x in json_data]\n",
    "with open (\"./result.json\", \"a\") as outfile:\n",
    "    json.dump(json_data, outfile)\n",
    "upload_file_to_minio('result.json', 'data', 'data-result/result.json', obj_storage_endpoint, obj_storage_access_key, obj_storage_secret_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T04:31:51.768854Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Update load result to MinIO with csv format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This file contain complex data such as dict, list.\n",
    "# So that, to save in csv format, we need convert these information to string.\n",
    "# To do that, I chose pandas lib to normalize json\n",
    "pd_data = pd.read_json(json.dumps(json_data))\n",
    "data_csv= pd_data.to_csv(\"result.csv\")\n",
    "# Now, upload file to MinIO\n",
    "upload_file_to_minio('result.csv', 'data', 'data-result/result.csv', obj_storage_endpoint, obj_storage_access_key, obj_storage_secret_key)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T04:31:51.769855Z"
    }
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
